# ç¬¬4ç« ï¼šé«˜çº§PromptæŠ€æœ¯

æœ¬ç« å°†æ·±å…¥æ¢è®¨é«˜çº§Promptå·¥ç¨‹æŠ€æœ¯ï¼Œå¸®åŠ©ä½ æ„å»ºæ›´å¼ºå¤§ã€æ›´å¯é çš„Agentã€‚

## 4.1 ç»“æ„åŒ–Promptè®¾è®¡

**ä¸ºä»€ä¹ˆè¦ç»“æ„åŒ–Promptï¼Ÿ**

æƒ³è±¡ä½ åœ¨ç»™ä¸€ä¸ªæ–°å‘˜å·¥å®‰æ’å·¥ä½œã€‚å¦‚æœä½ åªæ˜¯ç®€å•è¯´"å»æŠŠé‚£ä¸ªé¡¹ç›®åšäº†"ï¼Œå‘˜å·¥å¯èƒ½ä¼šä¸€è„¸èŒ«ç„¶ï¼Œä¸çŸ¥é“ä»ä½•ä¸‹æ‰‹ã€‚ä½†å¦‚æœä½ è¯´"ä½ æ˜¯é¡¹ç›®Açš„è´Ÿè´£äººï¼Œä¸»è¦ä»»åŠ¡æ˜¯å®ŒæˆXã€Yã€Zï¼Œæ³¨æ„è¦éµå¾ªä»¥ä¸‹è§„èŒƒï¼Œæœ€åç”¨è¿™ä¸ªæ ¼å¼æäº¤æŠ¥å‘Š"ï¼Œå‘˜å·¥å°±èƒ½æ¸…æ¥šåœ°çŸ¥é“è¯¥åšä»€ä¹ˆã€‚

ç»“æ„åŒ–Promptå°±æ˜¯è¿™ä¸ªé“ç†ã€‚é€šè¿‡æ¸…æ™°çš„ç»“æ„å‘Šè¯‰AIï¼š
- **ä½ æ˜¯è°**ï¼ˆè§’è‰²å®šä¹‰ï¼‰
- **è¦åšä»€ä¹ˆ**ï¼ˆä»»åŠ¡æè¿°ï¼‰
- **æ€ä¹ˆåš**ï¼ˆè¡Œä¸ºå‡†åˆ™ï¼‰
- **è¾“å‡ºä»€ä¹ˆæ ·**ï¼ˆæ ¼å¼è¦æ±‚ï¼‰

**ç»“æ„åŒ–Promptçš„æ ¸å¿ƒè¦ç´ **

ä¸€ä¸ªå¥½çš„ç»“æ„åŒ–Prompté€šå¸¸åŒ…å«è¿™å‡ ä¸ªéƒ¨åˆ†ï¼š

1. **è§’è‰²å®šä¹‰**ï¼šè®©AIçŸ¥é“å®ƒæ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Œæ¯”å¦‚"ä½ æ˜¯ä¸€ä¸ªèµ„æ·±Pythonå¼€å‘å·¥ç¨‹å¸ˆ"
2. **ä»»åŠ¡è¯´æ˜**ï¼šå…·ä½“è¦å®Œæˆä»€ä¹ˆä»»åŠ¡ï¼Œè¶Šè¯¦ç»†è¶Šå¥½
3. **è¡Œä¸ºå‡†åˆ™**ï¼šåº”è¯¥éµå¾ªä»€ä¹ˆè§„åˆ™ï¼Œåº”è¯¥é¿å…ä»€ä¹ˆè¡Œä¸º
4. **è¾“å‡ºæ ¼å¼**ï¼šå¸Œæœ›å¾—åˆ°ä»€ä¹ˆæ ¼å¼çš„è¾“å‡ºï¼Œæ¯”å¦‚JSONã€Markdownç­‰
5. **é™åˆ¶æ¡ä»¶**ï¼šå“ªäº›äº‹æƒ…ä¸èƒ½åšï¼Œå“ªäº›è¾¹ç•Œä¸èƒ½é€¾è¶Š

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡æœ‰æ•ˆï¼Ÿ**

å¤§è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒæ—¶è§è¿‡å¤§é‡ç»“æ„åŒ–çš„æ–‡æ¡£ï¼ˆå¦‚æŠ€æœ¯æ–‡æ¡£ã€æ•™ç¨‹ã€è§„èŒƒç­‰ï¼‰ï¼Œæ‰€ä»¥å®ƒèƒ½å¾ˆå¥½åœ°ç†è§£ç»“æ„åŒ–çš„æŒ‡ä»¤ã€‚ç»“æ„åŒ–çš„Promptå°±åƒç»™AIä¸€ä¸ªæ¸…æ™°çš„"å·¥ä½œæ‰‹å†Œ"ï¼Œè®©å®ƒçŸ¥é“ï¼š
- è¯¥å…³æ³¨ä»€ä¹ˆ
- è¯¥å¿½ç•¥ä»€ä¹ˆ
- è¯¥å¦‚ä½•ç»„ç»‡è¾“å‡º

### System Promptè®¾è®¡

System Promptæ˜¯å®šä¹‰Agentè¡Œä¸ºçš„å…³é”®ï¼š

```python
SYSTEM_PROMPT_TEMPLATE = """
# è§’è‰²å®šä¹‰
ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œä¸“æ³¨äº{domain}é¢†åŸŸã€‚

# æ ¸å¿ƒèƒ½åŠ›
{capabilities}

# è¡Œä¸ºå‡†åˆ™
{guidelines}

# è¾“å‡ºæ ¼å¼
{output_format}

# é™åˆ¶æ¡ä»¶
{constraints}
"""

def create_system_prompt(config):
    return SYSTEM_PROMPT_TEMPLATE.format(
        role=config.get('role', 'æ™ºèƒ½åŠ©æ‰‹'),
        domain=config.get('domain', 'é€šç”¨'),
        capabilities='\n'.join(f"- {c}" for c in config.get('capabilities', [])),
        guidelines='\n'.join(f"- {g}" for g in config.get('guidelines', [])),
        output_format=config.get('output_format', 'è‡ªç„¶è¯­è¨€'),
        constraints='\n'.join(f"- {c}" for c in config.get('constraints', []))
    )

# ç¤ºä¾‹é…ç½®
config = {
    'role': 'ä»£ç å®¡æŸ¥ä¸“å®¶',
    'domain': 'è½¯ä»¶å¼€å‘',
    'capabilities': [
        'ä»£ç è´¨é‡åˆ†æ',
        'å®‰å…¨æ¼æ´æ£€æµ‹',
        'æ€§èƒ½ä¼˜åŒ–å»ºè®®',
        'æœ€ä½³å®è·µæ¨è'
    ],
    'guidelines': [
        'ä¿æŒä¸“ä¸šã€å®¢è§‚çš„æ€åº¦',
        'æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®',
        'è§£é‡Šé—®é¢˜çš„åŸå› å’Œå½±å“',
        'ç»™å‡ºå¯æ‰§è¡Œçš„ä»£ç ç¤ºä¾‹'
    ],
    'output_format': '''
## é—®é¢˜åˆ—è¡¨
- [ä¸¥é‡ç¨‹åº¦] é—®é¢˜æè¿°
  - ä½ç½®ï¼šæ–‡ä»¶å:è¡Œå·
  - å»ºè®®ï¼šæ”¹è¿›å»ºè®®
  - ç¤ºä¾‹ï¼šä»£ç ç¤ºä¾‹

## æ€»ä½“è¯„ä»·
è¯„åˆ†å’Œæ€»ç»“
''',
    'constraints': [
        'ä¸ä¿®æ”¹ä¸šåŠ¡é€»è¾‘',
        'ä¸å¼•å…¥æ–°çš„ä¾èµ–',
        'ä¿æŒä»£ç é£æ ¼ä¸€è‡´'
    ]
}

system_prompt = create_system_prompt(config)
```

### è§’è‰²æ‰®æ¼”Prompt

é€šè¿‡è§’è‰²æ‰®æ¼”è®©Agentæ›´ä¸“ä¸šï¼š

```python
class RolePlayAgent:
    def __init__(self, role_config):
        self.client = OpenAI()
        self.role = role_config
        
        self.system_prompt = f"""
ä½ ç°åœ¨æ‰®æ¼”ï¼š{role_config['name']}

èƒŒæ™¯æ•…äº‹ï¼š
{role_config['background']}

æ€§æ ¼ç‰¹ç‚¹ï¼š
{role_config['personality']}

è¯´è¯é£æ ¼ï¼š
{role_config['speaking_style']}

ä¸“ä¸šé¢†åŸŸï¼š
{role_config['expertise']}

è¯·å§‹ç»ˆä¿æŒè§’è‰²ï¼Œç”¨ç¬¬ä¸€äººç§°å›ç­”é—®é¢˜ã€‚
"""
    
    def chat(self, user_message):
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_message}
            ]
        )
        return response.choices[0].message.content

# å®šä¹‰è§’è‰²
expert_role = {
    'name': 'å¼ æ•™æˆ',
    'background': 'æ¸…åå¤§å­¦è®¡ç®—æœºç³»æ•™æˆï¼Œä¸“æ³¨AIç ”ç©¶20å¹´',
    'personality': 'ä¸¥è°¨ã€è€å¿ƒã€å–„äºå¯å‘',
    'speaking_style': 'å­¦æœ¯ä½†é€šä¿—æ˜“æ‡‚ï¼Œå–œæ¬¢ç”¨æ¯”å–»',
    'expertise': 'æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†'
}

agent = RolePlayAgent(expert_role)
print(agent.chat("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"))
```

### è¾“å‡ºæ ¼å¼æ§åˆ¶

ç²¾ç¡®æ§åˆ¶è¾“å‡ºæ ¼å¼ï¼š

```python
import json

class StructuredOutputAgent:
    def __init__(self):
        self.client = OpenAI()
    
    def get_json_output(self, prompt, schema):
        """
        å¼ºåˆ¶JSONæ ¼å¼è¾“å‡º
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            schema: JSON Schemaå®šä¹‰
        """
        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ•°æ®æå–ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSON Schemaè¾“å‡ºï¼š

{json.dumps(schema, indent=2, ensure_ascii=False)}

è¦æ±‚ï¼š
1. åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
2. ä¸¥æ ¼éµå¾ªSchemaå®šä¹‰
3. å¿…å¡«å­—æ®µä¸èƒ½ç¼ºå¤±
"""
        
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0
        )
        
        content = response.choices[0].message.content
        
        # å°è¯•è§£æJSON
        try:
            # å»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            return json.loads(content.strip())
        except json.JSONDecodeError:
            return {"error": "æ— æ³•è§£æJSON", "raw": content}
    
    def get_markdown_output(self, prompt, template):
        """å¼ºåˆ¶Markdownæ ¼å¼è¾“å‡º"""
        system_prompt = f"""
è¯·æŒ‰ç…§ä»¥ä¸‹æ¨¡æ¿æ ¼å¼è¾“å‡ºï¼š

{template}

è¦æ±‚ï¼š
1. ä½¿ç”¨Markdownæ ¼å¼
2. ä¿æŒæ ‡é¢˜å±‚çº§æ­£ç¡®
3. ä»£ç å—ä½¿ç”¨æ­£ç¡®çš„è¯­æ³•é«˜äº®
"""
        
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
agent = StructuredOutputAgent()

# JSONè¾“å‡º
schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer"},
        "skills": {
            "type": "array",
            "items": {"type": "string"}
        }
    },
    "required": ["name", "age", "skills"]
}

result = agent.get_json_output(
    "æå–ä¿¡æ¯ï¼šå¼ ä¸‰ï¼Œ28å²ï¼Œæ“…é•¿Pythonã€JavaScriptã€Go",
    schema
)
print(json.dumps(result, indent=2, ensure_ascii=False))

# Markdownè¾“å‡º
template = """
# {title}

## æ¦‚è¿°
{overview}

## è¯¦ç»†è¯´æ˜
{details}

## ä»£ç ç¤ºä¾‹
python
{code}
"""

markdown = agent.get_markdown_output(
    "è§£é‡ŠPythonçš„åˆ—è¡¨æ¨å¯¼å¼",
    template
)
print(markdown)
```

## 4.2 é«˜çº§æ¨ç†æŠ€æœ¯

**ä¸ºä»€ä¹ˆéœ€è¦é«˜çº§æ¨ç†æŠ€æœ¯ï¼Ÿ**

å¤§è¯­è¨€æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†å¹¶ä¸æ˜¯æ¯æ¬¡éƒ½èƒ½ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚å°±åƒäººä¸€æ ·ï¼Œæœ‰æ—¶å€™ä¼š"æƒ³é”™"æˆ–è€…"æƒ³åäº†"ã€‚é«˜çº§æ¨ç†æŠ€æœ¯å°±æ˜¯é€šè¿‡ä¸€äº›ç­–ç•¥ï¼Œè®©AI"æƒ³å¾—æ›´æ¸…æ¥š"ã€"æƒ³å¾—æ›´å…¨é¢"ã€‚

**ä¸‰ç§æ ¸å¿ƒæ¨ç†æŠ€æœ¯**

1. **Self-Consistencyï¼ˆè‡ªä¸€è‡´æ€§ï¼‰**
   - **åŸç†**ï¼šå°±åƒ"ä¸‰ä¸ªè‡­çš®åŒ é¡¶ä¸ªè¯¸è‘›äº®"ï¼Œè®©AIå¯¹åŒä¸€ä¸ªé—®é¢˜æ€è€ƒå¤šæ¬¡ï¼Œç„¶åé€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆ
   - **é€‚ç”¨åœºæ™¯**ï¼šæ•°å­¦è®¡ç®—ã€é€»è¾‘æ¨ç†ã€é€‰æ‹©é¢˜ç­‰æœ‰æ˜ç¡®ç­”æ¡ˆçš„é—®é¢˜
   - **ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼šé€šè¿‡å¤šæ¬¡é‡‡æ ·ï¼Œé™ä½å¶ç„¶æ€§é”™è¯¯ï¼Œæé«˜ç­”æ¡ˆå¯é æ€§

2. **Tree of Thoughtsï¼ˆæ€ç»´æ ‘ï¼‰**
   - **åŸç†**ï¼šå°±åƒä¸‹æ£‹æ—¶æ€è€ƒ"å¦‚æœæˆ‘èµ°è¿™æ­¥ï¼Œå¯¹æ‰‹ä¼šæ€ä¹ˆèµ°ï¼Œç„¶åæˆ‘åˆè¯¥æ€ä¹ˆèµ°"ï¼Œæ¢ç´¢å¤šæ¡å¯èƒ½çš„æ¨ç†è·¯å¾„
   - **é€‚ç”¨åœºæ™¯**ï¼šå¤æ‚å†³ç­–ã€åˆ›æ„å†™ä½œã€é—®é¢˜è§£å†³ç­‰éœ€è¦å¤šæ­¥æ¨ç†çš„ä»»åŠ¡
   - **ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼šä¸å±€é™äºä¸€æ¡æ€è·¯ï¼Œå¯ä»¥æ‰¾åˆ°æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆ

3. **Self-Refineï¼ˆè‡ªæˆ‘ä¼˜åŒ–ï¼‰**
   - **åŸç†**ï¼šå°±åƒå†™ä½œæ–‡æ—¶çš„"å†™-æ”¹-å†™"å¾ªç¯ï¼Œå…ˆç”Ÿæˆåˆç¨¿ï¼Œç„¶åè‡ªæˆ‘æ‰¹è¯„ï¼Œå†æ”¹è¿›ï¼Œåå¤è¿­ä»£
   - **é€‚ç”¨åœºæ™¯**ï¼šä»£ç ç”Ÿæˆã€æ–‡ç« å†™ä½œã€æ–¹æ¡ˆè®¾è®¡ç­‰éœ€è¦ä¸æ–­ä¼˜åŒ–çš„ä»»åŠ¡
   - **ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼šé€šè¿‡è‡ªæˆ‘åæ€å’Œè¿­ä»£ï¼Œé€æ­¥æå‡è¾“å‡ºè´¨é‡

**å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨ç†æŠ€æœ¯ï¼Ÿ**

| æŠ€æœ¯ç±»å‹ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|---------|---------|------|------|
| Self-Consistency | æœ‰æ˜ç¡®ç­”æ¡ˆçš„é—®é¢˜ | ç®€å•æœ‰æ•ˆï¼Œå¯é æ€§é«˜ | æˆæœ¬è¾ƒé«˜ï¼ˆéœ€è¦å¤šæ¬¡è°ƒç”¨ï¼‰ |
| Tree of Thoughts | å¤æ‚æ¨ç†é—®é¢˜ | èƒ½æ‰¾åˆ°æ›´ä¼˜è§£ | å®ç°å¤æ‚ï¼Œè€—æ—¶è¾ƒé•¿ |
| Self-Refine | åˆ›é€ æ€§ä»»åŠ¡ | è´¨é‡æå‡æ˜æ˜¾ | éœ€è¦å¤šæ¬¡è¿­ä»£ |

### Self-Consistency

é€šè¿‡å¤šæ¬¡é‡‡æ ·æé«˜ç­”æ¡ˆå¯é æ€§ï¼š

```python
class SelfConsistencyAgent:
    def __init__(self, n_samples=5):
        self.client = OpenAI()
        self.n_samples = n_samples
    
    def generate_answers(self, question):
        """ç”Ÿæˆå¤šä¸ªç­”æ¡ˆ"""
        answers = []
        
        for _ in range(self.n_samples):
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{
                    "role": "user",
                    "content": f"""
è¯·ä¸€æ­¥æ­¥æ€è€ƒå¹¶å›ç­”é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š
"""
                }],
                temperature=0.7  # è¾ƒé«˜æ¸©åº¦å¢åŠ å¤šæ ·æ€§
            )
            answers.append(response.choices[0].message.content)
        
        return answers
    
    def find_consensus(self, answers):
        """æ‰¾åˆ°æœ€ä¸€è‡´çš„ç­”æ¡ˆ"""
        # ç®€å•çš„æŠ•ç¥¨æœºåˆ¶
        from collections import Counter
        
        # æå–æœ€ç»ˆç­”æ¡ˆ
        final_answers = []
        for answer in answers:
            # å‡è®¾ç­”æ¡ˆåœ¨æœ€åä¸€è¡Œ
            final_line = answer.strip().split('\n')[-1]
            final_answers.append(final_line)
        
        # ç»Ÿè®¡æœ€å¸¸è§çš„ç­”æ¡ˆ
        counter = Counter(final_answers)
        most_common = counter.most_common(1)[0]
        
        return most_common[0], most_common[1] / len(answers)
    
    def solve(self, question):
        """ä½¿ç”¨Self-Consistencyè§£å†³é—®é¢˜"""
        print(f"é—®é¢˜ï¼š{question}\n")
        
        # ç”Ÿæˆå¤šä¸ªç­”æ¡ˆ
        answers = self.generate_answers(question)
        
        print(f"ç”Ÿæˆäº† {len(answers)} ä¸ªç­”æ¡ˆï¼š")
        for i, answer in enumerate(answers, 1):
            print(f"\n--- ç­”æ¡ˆ {i} ---")
            print(answer[:200] + "..." if len(answer) > 200 else answer)
        
        # æ‰¾åˆ°å…±è¯†
        consensus, confidence = self.find_consensus(answers)
        
        print(f"\n=== æœ€ç»ˆç­”æ¡ˆ ===")
        print(f"ç­”æ¡ˆï¼š{consensus}")
        print(f"ç½®ä¿¡åº¦ï¼š{confidence:.1%}")
        
        return consensus, confidence

# ä½¿ç”¨
agent = SelfConsistencyAgent(n_samples=5)
agent.solve("ä¸€ä¸ªç¯®å­é‡Œæœ‰5ä¸ªè‹¹æœï¼Œæ‹¿èµ°2ä¸ªï¼Œåˆæ”¾è¿›3ä¸ªï¼Œé—®ç¯®å­é‡Œç°åœ¨æœ‰å‡ ä¸ªè‹¹æœï¼Ÿ")
```

### Tree of Thoughts

é€šè¿‡æ¢ç´¢å¤šæ¡æ¨ç†è·¯å¾„æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆï¼š

```python
class TreeOfThoughts:
    def __init__(self, max_depth=3, branching_factor=3):
        self.client = OpenAI()
        self.max_depth = max_depth
        self.branching_factor = branching_factor
    
    def generate_thoughts(self, state, n=3):
        """ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„æ€è€ƒæ–¹å‘"""
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": f"""
å½“å‰çŠ¶æ€ï¼š{state}

è¯·ç”Ÿæˆ {n} ä¸ªä¸åŒçš„æ€è€ƒæ–¹å‘æ¥æ¨è¿›é—®é¢˜è§£å†³ã€‚
æ¯ä¸ªæ€è€ƒæ–¹å‘åº”è¯¥ï¼š
1. æ˜¯ä¸€ä¸ªå…·ä½“çš„æ¨ç†æ­¥éª¤
2. ä¸å…¶ä»–æ–¹å‘ä¸åŒ
3. æœ‰åŠ©äºè§£å†³é—®é¢˜

ä»¥åˆ—è¡¨å½¢å¼è¾“å‡ºï¼Œæ¯è¡Œä¸€ä¸ªæ€è€ƒæ–¹å‘ã€‚
"""
            }],
            temperature=0.7
        )
        
        thoughts = response.choices[0].message.content.strip().split('\n')
        return [t.strip() for t in thoughts if t.strip()][:n]
    
    def evaluate_thought(self, thought, goal):
        """è¯„ä¼°æ€è€ƒæ–¹å‘çš„ä»·å€¼"""
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": f"""
ç›®æ ‡ï¼š{goal}

æ€è€ƒæ–¹å‘ï¼š{thought}

è¯·è¯„ä¼°è¿™ä¸ªæ€è€ƒæ–¹å‘å¯¹è¾¾æˆç›®æ ‡çš„ä»·å€¼ã€‚
è¾“å‡ºä¸€ä¸ª0-10çš„åˆ†æ•°ï¼Œ10è¡¨ç¤ºæœ€æœ‰ä»·å€¼ã€‚
åªè¾“å‡ºæ•°å­—ã€‚
"""
            }],
            temperature=0
        )
        
        try:
            return float(response.choices[0].message.content.strip())
        except:
            return 5.0
    
    def search(self, problem):
        """æœç´¢æœ€ä½³è§£å†³è·¯å¾„"""
        print(f"é—®é¢˜ï¼š{problem}\n")
        
        best_path = []
        best_score = 0
        current_state = problem
        
        for depth in range(self.max_depth):
            print(f"\n=== æ·±åº¦ {depth + 1} ===")
            
            # ç”Ÿæˆæ€è€ƒæ–¹å‘
            thoughts = self.generate_thoughts(current_state, self.branching_factor)
            
            # è¯„ä¼°æ¯ä¸ªæ–¹å‘
            scored_thoughts = []
            for thought in thoughts:
                score = self.evaluate_thought(thought, problem)
                scored_thoughts.append((thought, score))
                print(f"æ€è€ƒï¼š{thought[:50]}... åˆ†æ•°ï¼š{score}")
            
            # é€‰æ‹©æœ€ä½³æ–¹å‘
            scored_thoughts.sort(key=lambda x: x[1], reverse=True)
            best_thought, best_score = scored_thoughts[0]
            
            print(f"\né€‰æ‹©ï¼š{best_thought}")
            
            best_path.append(best_thought)
            current_state = f"{current_state}\nå·²æ€è€ƒï¼š{best_thought}"
            
            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ
            if best_score >= 9:
                print("\nâœ… æ‰¾åˆ°é«˜è´¨é‡è§£å†³æ–¹æ¡ˆï¼")
                break
        
        return best_path

# ä½¿ç”¨
tot = TreeOfThoughts(max_depth=3, branching_factor=3)
path = tot.search("å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ•ˆçš„ç¼“å­˜ç³»ç»Ÿï¼Ÿ")
print(f"\næœ€ç»ˆè§£å†³è·¯å¾„ï¼š{path}")
```

### Self-Refine

è‡ªæˆ‘è¿­ä»£ä¼˜åŒ–ï¼š

```python
class SelfRefineAgent:
    def __init__(self, max_iterations=3):
        self.client = OpenAI()
        self.max_iterations = max_iterations
    
    def generate(self, requirement):
        """ç”Ÿæˆåˆå§‹å†…å®¹"""
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": f"è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ç”Ÿæˆå†…å®¹ï¼š\n\n{requirement}"
            }]
        )
        return response.choices[0].message.content
    
    def critique(self, content, requirement):
        """æ‰¹è¯„å’Œæ”¹è¿›å»ºè®®"""
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": f"""
è¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹ï¼š

è¦æ±‚ï¼š
{requirement}

å†…å®¹ï¼š
{content}

è¯„ä¼°æ ‡å‡†ï¼š
1. æ˜¯å¦æ»¡è¶³æ‰€æœ‰è¦æ±‚
2. æœ‰å“ªäº›ä¸è¶³ä¹‹å¤„
3. å…·ä½“çš„æ”¹è¿›å»ºè®®

å¦‚æœå†…å®¹å®Œå…¨æ»¡è¶³è¦æ±‚ï¼Œè¾“å‡ºï¼šSATISFIED
å¦åˆ™ï¼Œè¾“å‡ºå…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
"""
            }]
        )
        return response.choices[0].message.content
    
    def refine(self, content, critique):
        """æ ¹æ®æ‰¹è¯„æ”¹è¿›å†…å®¹"""
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": f"""
è¯·æ ¹æ®æ”¹è¿›å»ºè®®ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ï¼š

åŸå†…å®¹ï¼š
{content}

æ”¹è¿›å»ºè®®ï¼š
{critique}

è¾“å‡ºä¼˜åŒ–åçš„å†…å®¹ã€‚
"""
            }]
        )
        return response.choices[0].message.content
    
    def run(self, requirement):
        """æ‰§è¡Œè‡ªæˆ‘ä¼˜åŒ–å¾ªç¯"""
        print(f"è¦æ±‚ï¼š{requirement}\n")
        
        content = self.generate(requirement)
        
        for i in range(self.max_iterations):
            print(f"\n=== è¿­ä»£ {i + 1} ===")
            print(f"å½“å‰å†…å®¹ï¼š\n{content[:200]}...\n")
            
            critique = self.critique(content, requirement)
            print(f"æ‰¹è¯„ï¼š\n{critique}\n")
            
            if "SATISFIED" in critique:
                print("âœ… å†…å®¹å·²æ»¡è¶³è¦æ±‚ï¼")
                return content
            
            content = self.refine(content, critique)
        
        print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°")
        return content

# ä½¿ç”¨
agent = SelfRefineAgent(max_iterations=3)
result = agent.run("å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œå®ç°å¿«é€Ÿæ’åºç®—æ³•ï¼Œè¦æ±‚ä»£ç ç®€æ´ã€æœ‰æ³¨é‡Šã€åŒ…å«æµ‹è¯•ç”¨ä¾‹")
```

## 4.3 Promptä¼˜åŒ–ç­–ç•¥

**ä¸ºä»€ä¹ˆéœ€è¦ä¼˜åŒ–Promptï¼Ÿ**

å†™å¥½ä¸€ä¸ªPromptå°±åƒå†™å¥½ä¸€æ®µä»£ç ï¼Œå¾ˆå°‘èƒ½ä¸€æ¬¡å°±å®Œç¾ã€‚ä½ å¯èƒ½ä¼šé‡åˆ°ï¼š
- AIç†è§£åäº†ï¼Œç»™å‡ºçš„ç­”æ¡ˆä¸æ˜¯ä½ æƒ³è¦çš„
- è¾“å‡ºæ ¼å¼ä¸ç¨³å®šï¼Œæœ‰æ—¶å€™å¯¹æœ‰æ—¶å€™é”™
- åœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°å¾ˆå¥½ï¼Œåœ¨å¦ä¸€äº›åœºæ™¯ä¸‹åˆä¸è¡Œ

Promptä¼˜åŒ–å°±æ˜¯é€šè¿‡ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼Œä¸æ–­æ”¹è¿›Promptï¼Œè®©å®ƒæ›´ç¨³å®šã€æ›´å¯é ã€‚

**ä¸‰ç§ä¼˜åŒ–æ–¹æ³•**

1. **è¿­ä»£ä¼˜åŒ–**
   - **æ€è·¯**ï¼šå°±åƒè°ƒè¯•ä»£ç ä¸€æ ·ï¼Œå‘ç°é—®é¢˜â†’åˆ†æåŸå› â†’ä¿®æ”¹â†’å†æµ‹è¯•
   - **æ­¥éª¤**ï¼š
     1. å‡†å¤‡æµ‹è¯•ç”¨ä¾‹ï¼ˆè¾“å…¥å’ŒæœŸæœ›è¾“å‡ºï¼‰
     2. è¿è¡ŒPromptï¼Œçœ‹å“ªäº›ç”¨ä¾‹å¤±è´¥äº†
     3. åˆ†æå¤±è´¥åŸå› 
     4. ä¿®æ”¹Prompt
     5. é‡å¤æµ‹è¯•ç›´åˆ°æ»¡æ„
   - **é€‚ç”¨åœºæ™¯**ï¼šæœ‰æ˜ç¡®æ­£ç¡®ç­”æ¡ˆçš„ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€æå–ã€è½¬æ¢ç­‰

2. **A/Bæµ‹è¯•**
   - **æ€è·¯**ï¼šå°±åƒäº§å“æµ‹è¯•ä¸€æ ·ï¼Œå¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„Promptå“ªä¸ªæ›´å¥½
   - **æ­¥éª¤**ï¼š
     1. å‡†å¤‡ä¸¤ä¸ªä¸åŒçš„Promptç‰ˆæœ¬
     2. ç”¨ç›¸åŒçš„æµ‹è¯•æ•°æ®åˆ†åˆ«è¿è¡Œ
     3. å¯¹æ¯”æ•ˆæœï¼ˆå‡†ç¡®ç‡ã€Tokenæ¶ˆè€—ã€å“åº”æ—¶é—´ç­‰ï¼‰
     4. é€‰æ‹©è¡¨ç°æ›´å¥½çš„ç‰ˆæœ¬
   - **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦é‡åŒ–å¯¹æ¯”çš„åœºæ™¯ï¼Œå¦‚é€‰æ‹©æœ€ä½³Promptæ¨¡æ¿

3. **ç‰ˆæœ¬ç®¡ç†**
   - **æ€è·¯**ï¼šå°±åƒä»£ç ç‰ˆæœ¬æ§åˆ¶ä¸€æ ·ï¼Œè®°å½•æ¯æ¬¡ä¿®æ”¹ï¼Œæ–¹ä¾¿å›æº¯
   - **å¥½å¤„**ï¼š
     - å¯ä»¥éšæ—¶å›é€€åˆ°ä¹‹å‰çš„ç‰ˆæœ¬
     - è®°å½•æ¯æ¬¡ä¿®æ”¹çš„åŸå› å’Œæ•ˆæœ
     - æ–¹ä¾¿å›¢é˜Ÿåä½œå’ŒçŸ¥è¯†ä¼ æ‰¿
   - **é€‚ç”¨åœºæ™¯**ï¼šé•¿æœŸç»´æŠ¤çš„Promptï¼Œéœ€è¦æŒç»­æ”¹è¿›

**ä¼˜åŒ–çš„å…³é”®åŸåˆ™**

- **å°æ­¥å¿«è·‘**ï¼šæ¯æ¬¡åªæ”¹ä¸€ä¸ªåœ°æ–¹ï¼Œçœ‹æ•ˆæœå†ç»§ç»­
- **è®°å½•è¿‡ç¨‹**ï¼šè®°å½•æ¯æ¬¡ä¿®æ”¹å’Œæ•ˆæœï¼Œæ–¹ä¾¿æ€»ç»“ç»éªŒ
- **å¤šåœºæ™¯æµ‹è¯•**ï¼šä¸è¦åªåœ¨å‡ ä¸ªä¾‹å­ä¸Šæµ‹è¯•ï¼Œè¦è¦†ç›–å„ç§æƒ…å†µ
- **å…³æ³¨å¤±è´¥æ¡ˆä¾‹**ï¼šå¤±è´¥æ¡ˆä¾‹å¾€å¾€èƒ½å‘ç°Promptçš„ä¸è¶³

### è¿­ä»£ä¼˜åŒ–æ–¹æ³•

ç³»ç»ŸåŒ–çš„Promptä¼˜åŒ–æµç¨‹ï¼š

```python
class PromptOptimizer:
    def __init__(self):
        self.client = OpenAI()
        self.history = []
    
    def test_prompt(self, prompt, test_cases):
        """æµ‹è¯•Promptæ•ˆæœ"""
        results = []
        
        for case in test_cases:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt.format(input=case['input'])}]
            )
            
            output = response.choices[0].message.content
            passed = self.evaluate_output(output, case['expected'])
            
            results.append({
                'input': case['input'],
                'expected': case['expected'],
                'output': output,
                'passed': passed
            })
        
        return results
    
    def evaluate_output(self, output, expected):
        """è¯„ä¼°è¾“å‡ºæ˜¯å¦æ­£ç¡®"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        if isinstance(expected, str):
            return expected.lower() in output.lower()
        elif isinstance(expected, list):
            return any(e.lower() in output.lower() for e in expected)
        return False
    
    def analyze_failures(self, results):
        """åˆ†æå¤±è´¥æ¡ˆä¾‹"""
        failures = [r for r in results if not r['passed']]
        
        if not failures:
            return "æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼"
        
        analysis = "å¤±è´¥æ¡ˆä¾‹åˆ†æï¼š\n"
        for f in failures:
            analysis += f"- è¾“å…¥ï¼š{f['input']}\n"
            analysis += f"  æœŸæœ›ï¼š{f['expected']}\n"
            analysis += f"  å®é™…ï¼š{f['output'][:100]}...\n\n"
        
        return analysis
    
    def improve_prompt(self, current_prompt, failures):
        """æ ¹æ®å¤±è´¥æ¡ˆä¾‹æ”¹è¿›Prompt"""
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": f"""
å½“å‰Promptï¼š
{current_prompt}

å¤±è´¥æ¡ˆä¾‹ï¼š
{failures}

è¯·åˆ†æå¤±è´¥åŸå› ï¼Œå¹¶æä¾›æ”¹è¿›åçš„Promptã€‚
åªè¾“å‡ºæ”¹è¿›åçš„Promptï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            }]
        )
        
        return response.choices[0].message.content
    
    def optimize(self, initial_prompt, test_cases, max_iterations=5):
        """ä¼˜åŒ–å¾ªç¯"""
        current_prompt = initial_prompt
        best_prompt = current_prompt
        best_score = 0
        
        for i in range(max_iterations):
            print(f"\n=== ä¼˜åŒ–è¿­ä»£ {i + 1} ===")
            
            # æµ‹è¯•
            results = self.test_prompt(current_prompt, test_cases)
            score = sum(1 for r in results if r['passed']) / len(results)
            
            print(f"å¾—åˆ†ï¼š{score:.1%}")
            
            # è®°å½•å†å²
            self.history.append({
                'iteration': i + 1,
                'prompt': current_prompt,
                'score': score
            })
            
            # æ›´æ–°æœ€ä½³
            if score > best_score:
                best_score = score
                best_prompt = current_prompt
            
            # å¦‚æœå…¨éƒ¨é€šè¿‡ï¼Œç»“æŸ
            if score == 1.0:
                print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
                break
            
            # åˆ†æå¤±è´¥å¹¶æ”¹è¿›
            failures = self.analyze_failures(results)
            current_prompt = self.improve_prompt(current_prompt, failures)
        
        return best_prompt, best_score

# ä½¿ç”¨ç¤ºä¾‹
test_cases = [
    {'input': 'ä»Šå¤©å¤©æ°”å¾ˆå¥½', 'expected': 'positive'},
    {'input': 'æœåŠ¡å¤ªå·®äº†', 'expected': 'negative'},
    {'input': 'è¿˜å¯ä»¥', 'expected': 'neutral'}
]

initial_prompt = "åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼š{input}"

optimizer = PromptOptimizer()
best_prompt, score = optimizer.optimize(initial_prompt, test_cases)
print(f"\næœ€ä½³Promptï¼š\n{best_prompt}")
print(f"æœ€ç»ˆå¾—åˆ†ï¼š{score:.1%}")
```

### A/Bæµ‹è¯•

å¯¹æ¯”ä¸åŒPromptçš„æ•ˆæœï¼š

```python
import random
from collections import defaultdict

class ABTestFramework:
    def __init__(self):
        self.client = OpenAI()
        self.results = defaultdict(list)
    
    def test_variant(self, prompt, inputs, variant_name):
        """æµ‹è¯•ä¸€ä¸ªå˜ä½“"""
        for input_text in inputs:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt.format(input=input_text)}]
            )
            
            self.results[variant_name].append({
                'input': input_text,
                'output': response.choices[0].message.content,
                'tokens': response.usage.total_tokens
            })
    
    def compare_variants(self, variant_a, variant_b, metric='quality'):
        """æ¯”è¾ƒä¸¤ä¸ªå˜ä½“"""
        results_a = self.results[variant_a]
        results_b = self.results[variant_b]
        
        if metric == 'quality':
            # äººå·¥è¯„ä¼°æˆ–è‡ªåŠ¨è¯„ä¼°
            pass
        elif metric == 'tokens':
            avg_a = sum(r['tokens'] for r in results_a) / len(results_a)
            avg_b = sum(r['tokens'] for r in results_b) / len(results_b)
            
            print(f"å¹³å‡Tokenæ¶ˆè€—ï¼š")
            print(f"  {variant_a}: {avg_a:.1f}")
            print(f"  {variant_b}: {avg_b:.1f}")
            print(f"  å·®å¼‚: {(avg_b - avg_a) / avg_a:.1%}")
    
    def run_ab_test(self, prompt_a, prompt_b, test_inputs):
        """è¿è¡ŒA/Bæµ‹è¯•"""
        print("è¿è¡ŒA/Bæµ‹è¯•...\n")
        
        self.test_variant(prompt_a, test_inputs, 'A')
        self.test_variant(prompt_b, test_inputs, 'B')
        
        self.compare_variants('A', 'B', 'tokens')
        
        return self.results
```

### Promptç‰ˆæœ¬ç®¡ç†

ç®¡ç†Promptçš„ä¸åŒç‰ˆæœ¬ï¼š

```python
import json
from datetime import datetime

class PromptVersionControl:
    def __init__(self, storage_path='prompts.json'):
        self.storage_path = storage_path
        self.versions = self.load()
    
    def load(self):
        """åŠ è½½ç‰ˆæœ¬å†å²"""
        try:
            with open(self.storage_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def save(self):
        """ä¿å­˜ç‰ˆæœ¬å†å²"""
        with open(self.storage_path, 'w') as f:
            json.dump(self.versions, f, indent=2, ensure_ascii=False)
    
    def add_version(self, name, prompt, metadata=None):
        """æ·»åŠ æ–°ç‰ˆæœ¬"""
        if name not in self.versions:
            self.versions[name] = []
        
        version_num = len(self.versions[name]) + 1
        version_id = f"{name}_v{version_num}"
        
        self.versions[name].append({
            'id': version_id,
            'prompt': prompt,
            'created_at': datetime.now().isoformat(),
            'metadata': metadata or {}
        })
        
        self.save()
        return version_id
    
    def get_version(self, name, version=None):
        """è·å–ç‰¹å®šç‰ˆæœ¬"""
        if name not in self.versions:
            return None
        
        versions = self.versions[name]
        
        if version is None:
            return versions[-1]  # æœ€æ–°ç‰ˆæœ¬
        
        for v in versions:
            if v['id'] == version:
                return v
        
        return None
    
    def list_versions(self, name):
        """åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬"""
        return self.versions.get(name, [])
    
    def compare_versions(self, name, v1, v2):
        """æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬"""
        ver1 = self.get_version(name, v1)
        ver2 = self.get_version(name, v2)
        
        if not ver1 or not ver2:
            return None
        
        return {
            'version1': ver1,
            'version2': ver2,
            'diff': self.compute_diff(ver1['prompt'], ver2['prompt'])
        }
    
    def compute_diff(self, text1, text2):
        """è®¡ç®—æ–‡æœ¬å·®å¼‚"""
        lines1 = text1.split('\n')
        lines2 = text2.split('\n')
        
        diff = []
        for i, (l1, l2) in enumerate(zip(lines1, lines2)):
            if l1 != l2:
                diff.append({
                    'line': i + 1,
                    'old': l1,
                    'new': l2
                })
        
        return diff

# ä½¿ç”¨
pvc = PromptVersionControl()

# æ·»åŠ ç‰ˆæœ¬
v1 = pvc.add_version(
    'sentiment_analysis',
    'åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼š{input}',
    {'author': 'developer', 'test_score': 0.85}
)

v2 = pvc.add_version(
    'sentiment_analysis',
    'è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆpositive/negative/neutralï¼‰ï¼š{input}',
    {'author': 'developer', 'test_score': 0.92}
)

# è·å–ç‰ˆæœ¬
current = pvc.get_version('sentiment_analysis')
print(f"å½“å‰ç‰ˆæœ¬ï¼š{current['id']}")
print(f"Promptï¼š{current['prompt']}")
```

## 4.4 ã€å®æˆ˜ã€‘ä»£ç å®¡æŸ¥Agent

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥Agentï¼Œç»¼åˆè¿ç”¨æœ¬ç« æ‰€å­¦æŠ€æœ¯ã€‚

### é¡¹ç›®ç»“æ„

```
code-review-agent/
â”œâ”€â”€ .env
â”œâ”€â”€ main.py
â”œâ”€â”€ agent.py
â”œâ”€â”€ prompts.py
â””â”€â”€ requirements.txt
```

### å®Œæ•´ä»£ç 

**prompts.py**

```python
SYSTEM_PROMPT = """
# è§’è‰²å®šä¹‰
ä½ æ˜¯ä¸€ä½èµ„æ·±ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œæ‹¥æœ‰15å¹´è½¯ä»¶å¼€å‘ç»éªŒã€‚

# æ ¸å¿ƒèƒ½åŠ›
- ä»£ç è´¨é‡åˆ†æ
- å®‰å…¨æ¼æ´æ£€æµ‹
- æ€§èƒ½é—®é¢˜è¯†åˆ«
- æœ€ä½³å®è·µæ¨è
- ä»£ç é£æ ¼æ£€æŸ¥

# å®¡æŸ¥æ ‡å‡†
1. ä»£ç æ­£ç¡®æ€§ï¼šé€»è¾‘æ˜¯å¦æ­£ç¡®ï¼Œè¾¹ç•Œæ¡ä»¶æ˜¯å¦å¤„ç†
2. ä»£ç å¯è¯»æ€§ï¼šå‘½åæ˜¯å¦æ¸…æ™°ï¼Œç»“æ„æ˜¯å¦åˆç†
3. ä»£ç å®‰å…¨æ€§ï¼šæ˜¯å¦å­˜åœ¨å®‰å…¨æ¼æ´
4. ä»£ç æ€§èƒ½ï¼šæ˜¯å¦å­˜åœ¨æ€§èƒ½é—®é¢˜
5. ä»£ç å¯ç»´æŠ¤æ€§ï¼šæ˜¯å¦æ˜“äºä¿®æ”¹å’Œæ‰©å±•

# è¾“å‡ºæ ¼å¼
## é—®é¢˜åˆ—è¡¨

### ä¸¥é‡é—®é¢˜ ğŸ”´
- [é—®é¢˜ç±»å‹] æè¿°
  - ä½ç½®ï¼šè¡Œå·
  - å½±å“ï¼šé—®é¢˜å½±å“
  - å»ºè®®ï¼šæ”¹è¿›å»ºè®®
  - ç¤ºä¾‹ï¼š
    ```python
    # æ”¹è¿›åçš„ä»£ç 
    ```

### ä¸€èˆ¬é—®é¢˜ ğŸŸ¡
- [é—®é¢˜ç±»å‹] æè¿°
  - å»ºè®®ï¼šæ”¹è¿›å»ºè®®

### ä¼˜åŒ–å»ºè®® ğŸŸ¢
- [å»ºè®®ç±»å‹] æè¿°
  - è¯´æ˜ï¼šä¸ºä»€ä¹ˆå»ºè®®è¿™æ ·æ”¹

## æ€»ä½“è¯„ä»·
- ä»£ç è´¨é‡è¯„åˆ†ï¼šX/10
- ä¸»è¦ä¼˜ç‚¹ï¼š
- éœ€è¦æ”¹è¿›ï¼š
- æ€»ç»“ï¼š
"""

REVIEW_TEMPLATE = """
è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼š

```{language}
{code}
```

å®¡æŸ¥é‡ç‚¹ï¼š{focus_areas}
"""
```

**agent.py**

```python
import os
import json
from openai import OpenAI
from prompts import SYSTEM_PROMPT, REVIEW_TEMPLATE

class CodeReviewAgent:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.review_history = []
    
    def review(self, code, language='python', focus_areas='all'):
        """æ‰§è¡Œä»£ç å®¡æŸ¥"""
        prompt = REVIEW_TEMPLATE.format(
            language=language,
            code=code,
            focus_areas=focus_areas
        )
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        review_result = response.choices[0].message.content
        
        # è®°å½•å†å²
        self.review_history.append({
            'code': code,
            'language': language,
            'review': review_result
        })
        
        return review_result
    
    def quick_check(self, code):
        """å¿«é€Ÿæ£€æŸ¥"""
        prompt = f"""
å¿«é€Ÿæ£€æŸ¥ä»¥ä¸‹ä»£ç çš„ä¸»è¦é—®é¢˜ï¼š

```
{code}
```

åªåˆ—å‡ºæœ€é‡è¦çš„3ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜ç”¨ä¸€å¥è¯æè¿°ã€‚
"""
        
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä»£ç å®¡æŸ¥åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content
    
    def suggest_fix(self, code, issue):
        """é’ˆå¯¹ç‰¹å®šé—®é¢˜æä¾›ä¿®å¤å»ºè®®"""
        prompt = f"""
ä»£ç ï¼š
```
{code}
```

é—®é¢˜ï¼š{issue}

è¯·æä¾›ï¼š
1. é—®é¢˜åˆ†æ
2. ä¿®å¤æ–¹æ¡ˆ
3. ä¿®å¤åçš„ä»£ç 
"""
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content
    
    def compare_versions(self, old_code, new_code):
        """æ¯”è¾ƒä»£ç ç‰ˆæœ¬"""
        prompt = f"""
æ¯”è¾ƒä¸¤ä¸ªä»£ç ç‰ˆæœ¬ï¼š

æ—§ç‰ˆæœ¬ï¼š
```
{old_code}
```

æ–°ç‰ˆæœ¬ï¼š
```
{new_code}
```

è¯·åˆ†æï¼š
1. æ”¹è¿›äº†å“ªäº›é—®é¢˜
2. æ˜¯å¦å¼•å…¥äº†æ–°é—®é¢˜
3. æ•´ä½“è´¨é‡å˜åŒ–
"""
        
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content
```

**main.py**

```python
from dotenv import load_dotenv
from agent import CodeReviewAgent

load_dotenv()

def main():
    agent = CodeReviewAgent()
    
    print("=" * 60)
    print("ä»£ç å®¡æŸ¥Agent")
    print("=" * 60)
    print("\nå‘½ä»¤ï¼š")
    print("  review - å®Œæ•´å®¡æŸ¥")
    print("  quick  - å¿«é€Ÿæ£€æŸ¥")
    print("  fix    - ä¿®å¤å»ºè®®")
    print("  quit   - é€€å‡º")
    print("=" * 60)
    
    while True:
        command = input("\nå‘½ä»¤: ").strip().lower()
        
        if command == 'quit':
            print("å†è§ï¼")
            break
        
        elif command == 'review':
            print("\nè¯·è¾“å…¥ä»£ç ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
            lines = []
            while True:
                line = input()
                if not line:
                    break
                lines.append(line)
            
            code = '\n'.join(lines)
            print("\næ­£åœ¨å®¡æŸ¥...\n")
            result = agent.review(code)
            print(result)
        
        elif command == 'quick':
            print("\nè¯·è¾“å…¥ä»£ç ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
            lines = []
            while True:
                line = input()
                if not line:
                    break
                lines.append(line)
            
            code = '\n'.join(lines)
            print("\nå¿«é€Ÿæ£€æŸ¥ç»“æœï¼š\n")
            result = agent.quick_check(code)
            print(result)
        
        elif command == 'fix':
            print("\nè¯·è¾“å…¥ä»£ç ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
            lines = []
            while True:
                line = input()
                if not line:
                    break
                lines.append(line)
            
            code = '\n'.join(lines)
            issue = input("\nè¯·æè¿°é—®é¢˜ï¼š")
            
            print("\nç”Ÿæˆä¿®å¤å»ºè®®...\n")
            result = agent.suggest_fix(code, issue)
            print(result)
        
        else:
            print("æœªçŸ¥å‘½ä»¤ï¼Œè¯·é‡è¯•ã€‚")

if __name__ == "__main__":
    main()
```

### è¿è¡Œç¤ºä¾‹

```bash
python main.py

å‘½ä»¤: quick

è¯·è¾“å…¥ä»£ç ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š
def calc(x,y):
    return x+y

å¿«é€Ÿæ£€æŸ¥ç»“æœï¼š

1. å‡½æ•°åä¸å¤Ÿæè¿°æ€§ï¼Œå»ºè®®ä½¿ç”¨æ›´æœ‰æ„ä¹‰çš„åç§°
2. ç¼ºå°‘ç±»å‹æ³¨è§£ï¼Œå»ºè®®æ·»åŠ å‚æ•°å’Œè¿”å›å€¼ç±»å‹
3. ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå»ºè®®æ·»åŠ å‡½æ•°è¯´æ˜
```

## æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

- âœ… ç»“æ„åŒ–Promptè®¾è®¡æ–¹æ³•
- âœ… è§’è‰²æ‰®æ¼”å’Œè¾“å‡ºæ ¼å¼æ§åˆ¶
- âœ… é«˜çº§æ¨ç†æŠ€æœ¯ï¼šSelf-Consistencyã€Tree of Thoughtsã€Self-Refine
- âœ… Promptä¼˜åŒ–ç­–ç•¥å’Œç‰ˆæœ¬ç®¡ç†
- âœ… æ„å»ºäº†ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥Agent

## ä¸‹ä¸€ç« 

ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ Function Callingä¸å·¥å…·ä½¿ç”¨ã€‚

[ç¬¬5ç« ï¼šFunction Callingä¸å·¥å…·ä½¿ç”¨ â†’](/core-skills/chapter5)
